{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils as tutils\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.utils as tvutils\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "sys.path.append(\".\")\n",
    "from models.naive_dcgan import *\n",
    "#from LSTM import LSTM\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    # Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = \"./dataset/\"\n",
    "image_size = 64\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "num_gf = 64\n",
    "num_df = 64\n",
    "epochs = 25\n",
    "num_row = 8\n",
    "n_sample_per_n_batches = 100\n",
    "z_dim = 100\n",
    "num_channels = 3\n",
    "num_gpu = 1\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda:0\")\n",
    "l_r = 0.0005\n",
    "beta1 = 0.5\n",
    "output_dir = \".\"\n",
    "netG_path = \"\"\n",
    "netD_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.makedirs(output_dir)\n",
    "except OSError:\n",
    "    pass\n",
    "dataset = datasets.ImageFolder(dataset_root, transform=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]))\n",
    "print(len(dataset))\n",
    "assert dataset\n",
    "dataloader = tutils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Info:\n",
      "Generator(\n",
      "  (net): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator Info:\n",
      "Discriminator(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "generator = Generator(z_dim, num_gf, num_channels, use_cuda, num_gpu)\n",
    "discriminator = Discriminator(num_channels, num_df, use_cuda, num_gpu)\n",
    "# move generator and disciminator to cuda device if cuda is activated\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "# init weights of  g and d\n",
    "\n",
    "generator.apply(init_weights)\n",
    "discriminator.apply(init_weights)\n",
    "\n",
    "# load net state if exists\n",
    "if netG_path != \"\":\n",
    "    generator.load_state_dict(torch.load(netG_path))\n",
    "if netD_path != \"\":\n",
    "    discriminator.load_state_dict(torch.load(netD_path))\n",
    "\n",
    "print(\"Generator Info:\")\n",
    "print(generator)\n",
    "print(\"Discriminator Info:\")\n",
    "print(discriminator)\n",
    "\n",
    "criteria = nn.BCELoss()\n",
    "\n",
    "# every time we will use this to sample from generator, so that we can compare the performance of training\n",
    "sample_batch_z = torch.randn(batch_size, z_dim, 1, 1).to(device)\n",
    "\n",
    "label_real = 1\n",
    "label_fake = 0\n",
    "\n",
    "optim_G = optim.Adam(generator.parameters(), lr=l_r,\n",
    "                     betas=(beta1, 0.999))\n",
    "optim_D = optim.Adam(discriminator.parameters(),\n",
    "                     lr=l_r*0.5, betas=(beta1, 0.999))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(D_real_loss,D_fake_loss,G_loss,st):\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    #plt.title(\"test loss with epoches \")\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('batch')\n",
    "    plt.plot(D_real_loss, label='real_loss', color='red')\n",
    "    plt.plot(D_fake_loss,label='fake_loss',color = 'green')\n",
    "    plt.plot(G_loss, label='G_loss', color='blue')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(st)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_fake_loss = []\n",
    "D_real_loss = []\n",
    "G_loss = []\n",
    "\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        real_loss = []\n",
    "        fake_loss = []\n",
    "        total_loss = []\n",
    "        G_batch_loss = []\n",
    "        \n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            # first update discriminator's parameters\n",
    "            # max(log(D(x))+log(1-D(G(z))))\n",
    "            # ------------------------------\n",
    "            # first train for real images\n",
    "\n",
    "            # zero grad\n",
    "            discriminator.zero_grad()\n",
    "            real_batch_data = data[0].to(device)\n",
    "            batch_size = real_batch_data.size(0)\n",
    "            batch_labels = torch.full((batch_size,), label_real, device=device)\n",
    "            # forward to get output of D for real images\n",
    "            real_batch_output = discriminator(real_batch_data)\n",
    "            # compute loss for D_real\n",
    "            loss_D_real = criteria(real_batch_output, batch_labels)\n",
    "            # backward the gradients\n",
    "            loss_D_real.backward()\n",
    "\n",
    "            loss_D_real_mean = loss_D_real.mean().item()\n",
    "            D_real_loss.append(loss_D_real_mean)\n",
    "            real_loss.append(loss_D_real_mean)\n",
    "\n",
    "            # now we train with fake images generated by G\n",
    "            # first sample from latent variable z\n",
    "            fake_batch_z = torch.randn(batch_size, z_dim, 1, 1, device=device)\n",
    "            # then generate fake images\n",
    "            fake_batch_images = generator(fake_batch_z)\n",
    "            # generate fake labels\n",
    "            batch_labels.fill_(label_fake)\n",
    "            # forward D to get output\n",
    "            # !这里使用detach,是为了防止判别网络通过生成网络的输出值拷贝改变其值,进而影响生成网络的梯度计算\n",
    "            fake_batch_output = discriminator(fake_batch_images.detach())\n",
    "            # compute loss of D for fake images\n",
    "            loss_D_fake = criteria(fake_batch_output, batch_labels)\n",
    "            # backward\n",
    "            loss_D_fake.backward()\n",
    "\n",
    "            loss_D_fake_mean = loss_D_fake.mean().item()\n",
    "            D_fake_loss.append(loss_D_fake_mean)\n",
    "            fake_loss.append(loss_D_fake_mean)\n",
    "\n",
    "            loss_D_total = loss_D_fake + loss_D_real\n",
    "            total_loss.append(loss_D_total)\n",
    "\n",
    "            # update G parameters\n",
    "            optim_D.step()\n",
    "\n",
    "            # now it's G's turn\n",
    "            # ---------------------------------\n",
    "            # let's maximize logD(G(z))\n",
    "            generator.zero_grad()\n",
    "            # mark label real for G\n",
    "            batch_labels.fill_(label_real)\n",
    "            # 我们需要判别网络更新之后的判别结果,就像银行知道了假币的缺陷,那么制造假币的需要知道银行掌握的信息(判据)\n",
    "            batch_output_G = discriminator(fake_batch_images)\n",
    "            # compute loss for G\n",
    "            loss_G = criteria(batch_output_G, batch_labels)\n",
    "            # compute gradient for G\n",
    "            loss_G.backward()\n",
    "\n",
    "            loss_G_mean = loss_G.mean().item()\n",
    "            G_loss.append(loss_G_mean)\n",
    "            G_batch_loss.append(loss_G_mean)\n",
    "\n",
    "            # update parameters for G\n",
    "            optim_G.step()\n",
    "\n",
    "            print(\"epoch:{}/{},n_batch:{}/{},loss_real_D:{:.4},loss_fake_D:{:.4},loss_G:{:.4}\".format(\n",
    "                epoch, epochs, i, len(dataloader), loss_D_real_mean, loss_D_fake_mean, loss_G_mean))\n",
    "\n",
    "            if i % n_sample_per_n_batches == 0:\n",
    "                # sample every 100 batch\n",
    "                tvutils.save_image(\n",
    "                    real_batch_data, \"%s/sample_real.png\" % output_dir, nrow=nrow,normalize=True)\n",
    "                fake_batch_images = generator(sample_batch_z)\n",
    "                tvutils.save_image(fake_batch_images.detach(), \"%s/sample_fake_images_epoch%03d_%s.png\" %\n",
    "                                   (output_dir, epoch,dataset),nrow=nrow, normalize=True)\n",
    "\n",
    "        torch.save(discriminator.state_dict(),\n",
    "                   \"%s/discriminator_epoch_%03d.pth\" % (output_dir, epoch))\n",
    "        torch.save(generator.state_dict(),\n",
    "                   \"%s/generator_epoch_%03d.pth\" % (output_dir, epoch))\n",
    "        plot(real_loss,fake_loss,G_loss,\"loss_epoch%03d_%s\"%  epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0/25,n_batch:0/157,loss_real_D:0.7191,loss_fake_D:0.8674,loss_G:3.235\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './sample_fake_images_epoch000_Dataset ImageFolder\\n    Number of datapoints: 10000\\n    Root Location: ./dataset/\\n    Transforms (if any): Compose(\\n                             Resize(size=64, interpolation=PIL.Image.BILINEAR)\\n                             CenterCrop(size=(64, 64))\\n                             ToTensor()\\n                             Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\\n                         )\\n    Target Transforms (if any): None.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-979718b0d63a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-8677c266c3d8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mfake_batch_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_batch_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 tvutils.save_image(fake_batch_images.detach(), \"%s/sample_fake_images_epoch%03d_%s.png\" %\n\u001b[0;32m---> 88\u001b[0;31m                                    (output_dir, epoch,dataset),nrow=nrow, normalize=True)\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         torch.save(discriminator.state_dict(),\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision/utils.py\u001b[0m in \u001b[0;36msave_image\u001b[0;34m(tensor, filename, nrow, padding, normalize, range, scale_each, pad_value)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mndarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   1989\u001b[0m                 \u001b[0;31m# Open also for reading (\"+\"), because TIFF save_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1990\u001b[0m                 \u001b[0;31m# writer needs to go back and edit the written data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1991\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1993\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './sample_fake_images_epoch000_Dataset ImageFolder\\n    Number of datapoints: 10000\\n    Root Location: ./dataset/\\n    Transforms (if any): Compose(\\n                             Resize(size=64, interpolation=PIL.Image.BILINEAR)\\n                             CenterCrop(size=(64, 64))\\n                             ToTensor()\\n                             Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\\n                         )\\n    Target Transforms (if any): None.png'"
     ]
    }
   ],
   "source": [
    "train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"D_real_loss.txt\", np.array(D_real_loss))\n",
    "np.savetxt(\"D_fake_loss.txt\", np.array(D_fake_loss))\n",
    "np.savetxt(\"G_loss.txt\", np.array(G_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
