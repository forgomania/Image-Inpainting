{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "from models.naive_dcgan import Generator, Discriminator, init_weights\n",
    "from utils.image_utils import get_tensor_image, save_tensor_images\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils as tutils\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.utils as tvutils\n",
    "import numpy as np\n",
    "from cv2 import imread, resize\n",
    "\n",
    "import random\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    # Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "load trained state dict from local files...\n",
      "generator and discriminator state dict loaded, done.\n",
      "Generator Info:\n",
      "Generator(\n",
      "  (net): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator Info:\n",
      "Discriminator(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "num_gpu = 1\n",
    "num_gf = 64\n",
    "num_df = 64\n",
    "num_iters = 1000\n",
    "lr = 0.01\n",
    "num_iters = 1000\n",
    "z_dim = 100\n",
    "lamd = 0.1\n",
    "image_size = 64\n",
    "batch_size = 64\n",
    "# number of image channels\n",
    "num_channels = 3\n",
    "output_dir = \"./results\"\n",
    "netG_path = \"./generator_epoch_000.pth\"\n",
    "netD_path = \"./discriminator_epoch_000.pth\"\n",
    "aligned_images = \"./dataset\" #需要进行impainting的测试图像集合\n",
    "use_cuda = True\n",
    "########need to add path#########\n",
    "aligned_images = []\n",
    "for r,d,f in os.walk(\"./dataset/img_align_celeba/\"):\n",
    "    aligned_images = f\n",
    "    \n",
    "print(len(aligned_images))\n",
    "assert aligned_images\n",
    "\n",
    "\n",
    "\n",
    "#################\n",
    "\n",
    "# set cudnn benchmark=True ,for the best of the backend to find best hardware algorithms\n",
    "cudnn.benchmark = True\n",
    "\n",
    "generator = Generator(z_dim, num_gf, num_channels, use_cuda, num_gpu)\n",
    "discriminator = Discriminator(num_channels, num_df, use_cuda, num_gpu)\n",
    "\n",
    "# move generator and disciminator to cuda device if cuda is activated\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "# init weights of  g and d\n",
    "\n",
    "generator.apply(init_weights)\n",
    "discriminator.apply(init_weights)\n",
    "\n",
    "# load net state if exists\n",
    "print(\"load trained state dict from local files...\")\n",
    "generator.load_state_dict(torch.load(netG_path))\n",
    "discriminator.load_state_dict(torch.load(netD_path))\n",
    "print(\"generator and discriminator state dict loaded, done.\")\n",
    "\n",
    "print(\"Generator Info:\")\n",
    "print(generator)\n",
    "print(\"Discriminator Info:\")\n",
    "print(discriminator)\n",
    "\n",
    "image_shape = [num_channels, image_size, image_size]\n",
    "\n",
    "criteria = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_images = aligned_images[0:10]\n",
    "for i in range(len(aligned_images)):\n",
    "    aligned_images[i] = \"./dataset/img_align_celeba/\" + aligned_images[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.resize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impainting():\n",
    "    # load mask\n",
    "    mask_img = imread(\"./qd_imd/train/14508_train.png\")\n",
    "    mask_img_ds = resize(mask_img, (64,64))\n",
    "    # 创建输出文件夹\n",
    "    output_dirs = output_dir\n",
    "    source_imagedir = os.path.join(output_dirs, \"source_images\")\n",
    "    masked_imagedir = os.path.join(output_dirs, \"masked_images\")\n",
    "    impainted_imagedir = os.path.join(output_dirs, \"impainted_images\")\n",
    "    os.makedirs(source_imagedir, exist_ok=True)\n",
    "    os.makedirs(masked_imagedir,exist_ok=True)\n",
    "    os.makedirs(impainted_imagedir,exist_ok=True)\n",
    "\n",
    "    # 总共需要修复多少图片\n",
    "    num_images = len(aligned_images)\n",
    "    # 总共可以分为多少的batch来进行处理\n",
    "    num_batches = int(np.ceil(num_images / batch_size))\n",
    "\n",
    "    for idx in range(num_batches):\n",
    "        # 对于每一个batch的图片进行如下处理\n",
    "        lidx = idx * batch_size\n",
    "        hidx = min(num_images, (idx + 1) * batch_size)\n",
    "        realBatchSize = hidx - lidx\n",
    "\n",
    "        batch_images = [get_tensor_image(imgpath) for imgpath in aligned_images[lidx:hidx]]\n",
    "        batch_images = torch.stack(batch_images).to(device)\n",
    "        # if realBatchSize < args.batch_size:\n",
    "        #     print(\"number of batch images : \", realBatchSize)\n",
    "        #     # 如果需要修补的图片没有一个batch那么多，用0来填充\n",
    "        #     batch_images = np.pad(batch_images, [(0, args.batch_size - realBatchSize), (0, 0), (0, 0), (0, 0)], \"constant\")\n",
    "        #     batch_images = batch_images.astype(np.float32)\n",
    "        \n",
    "        # 输入的原始图片已经准备好，开始准备mask\n",
    "        # 暂时只提供中心mask\n",
    "        mask = torch.ones(size=image_shape).to(device)\n",
    "        imageCenterScale = 0.3\n",
    "        lm = int(image_size * imageCenterScale)\n",
    "        hm = int(image_size * (1 - imageCenterScale))\n",
    "        # 将图像中心mask为0\n",
    "        mask[:,lm:hm, lm:hm] = 0.0\n",
    "        #print(batch_images.shape)\n",
    "        #print(mask.shape)\n",
    "        #return batch_images,mask\n",
    "        #masked_batch_images = torch.mul(batch_images, mask).to(device)\n",
    "        images[:,:,32:96,32:96]=mask\n",
    "        masked_batch_images=images.to(device)\n",
    "\n",
    "        # 先保存一下原始图片和masked图片\n",
    "        save_tensor_images(batch_images.detach(),\n",
    "                   os.path.join(source_imagedir,\"source_image_batch_{}.png\".format(idx)))\n",
    "    \n",
    "        save_tensor_images(masked_batch_images.detach(), os.path.join(masked_imagedir, \"masked_image_batch_{}.png\".format(idx)))\n",
    "\n",
    "       \n",
    "        z_hat = torch.rand(size=[realBatchSize,z_dim,1,1],dtype=torch.float32,requires_grad=True,device=device)\n",
    "        z_hat.data.mul_(2.0).sub_(1.0)\n",
    "        opt = optim.Adam([z_hat],lr=0.0001)       \n",
    "        print(\"start impainting iteration for batch : {}\".format(idx))\n",
    "        v=torch.tensor(0,dtype=torch.float32,device=device)\n",
    "        m=torch.tensor(0,dtype=torch.float32,device=device)\n",
    "        \n",
    "        for iteration in range(num_iters):\n",
    "            # 对每一个batch的图像分别迭代impainting\n",
    "            if z_hat.grad is not None:\n",
    "                z_hat.grad.data.zero_()\n",
    "            generator.zero_grad()\n",
    "            discriminator.zero_grad()\n",
    "            batch_images_g = generator(z_hat)\n",
    "            batch_images_g_masked = torch.mul(batch_images_g,mask) \n",
    "            impainting_images = torch.mul(batch_images_g,(1-mask))+masked_batch_images\n",
    "            if iteration % 100==0:\n",
    "                # 保存impainting 图片结果\n",
    "                print(\"\\nsaving impainted images for batch: {} , iteration:{}\".format(idx,iteration))\n",
    "                save_tensor_images(impainting_images.detach(), os.path.join(impainted_imagedir,\"impainted_image_batch_{}_iteration_{}.png\".format(idx,iteration)))\n",
    "\n",
    "            loss_context = torch.norm(\n",
    "                (masked_batch_images-batch_images_g_masked),p=1)\n",
    "            dis_output = discriminator(impainting_images)\n",
    "#             print(dis_output)\n",
    "            batch_labels = torch.full((realBatchSize,), 1, device=device)\n",
    "            loss_perceptual = criteria(dis_output,batch_labels)\n",
    "            \n",
    "            total_loss = loss_context + lamd*loss_perceptual\n",
    "            print(\"\\r batch {} : iteration : {:4} , context_loss:{:.4f},percptual_loss:{:4f}\".format(idx,iteration,loss_context,loss_perceptual),end=\"\")\n",
    "            total_loss.backward()\n",
    "            opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 218, 178])\n",
      "torch.Size([3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "images,mask=impainting()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[:,:,32:96,32:96]=mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start impainting iteration for batch : 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (178) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-73075458021f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimpainting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-62218d6ddf6b>\u001b[0m in \u001b[0;36mimpainting\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mbatch_images_g\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mbatch_images_g_masked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_images_g\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0mimpainting_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_images_g\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmasked_batch_images\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[1;31m# 保存impainting 图片结果\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (178) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "impainting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=torch.from_numpy(np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[2]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1], dtype=torch.int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "s_img = cv2.imread(\"smaller_image.png\")\n",
    "l_img = cv2.imread(\"larger_image.jpg\")\n",
    "x_offset=y_offset=50\n",
    "l_img[y_offset:y_offset+s_img.shape[0], x_offset:x_offset+s_img.shape[1]] = s_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
