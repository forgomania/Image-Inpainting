{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "from models.naive_dcgan import Generator, Discriminator, init_weights\n",
    "from utils.image_utils import get_tensor_image, save_tensor_images\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils as tutils\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.utils as tvutils\n",
    "import numpy as np\n",
    "from cv2 import imread, resize\n",
    "\n",
    "import random\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    # Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69819\n",
      "load trained state dict from local files...\n",
      "generator and discriminator state dict loaded, done.\n",
      "Generator Info:\n",
      "Generator(\n",
      "  (net): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator Info:\n",
      "Discriminator(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "num_gpu = 1\n",
    "num_gf = 64\n",
    "num_df = 64\n",
    "num_iters = 1000\n",
    "lr = 0.01\n",
    "num_iters = 1000\n",
    "z_dim = 100\n",
    "lamd = 0.1\n",
    "image_size = 64\n",
    "batch_size = 64\n",
    "# number of image channels\n",
    "num_channels = 3\n",
    "output_dir = \"./results\"\n",
    "netG_path = \"./generator_epoch_024.pth\"\n",
    "netD_path = \"./discriminator_epoch_024.pth\"\n",
    "aligned_images = \"./dataset\" #需要进行impainting的测试图像集合\n",
    "use_cuda = True\n",
    "########need to add path#########\n",
    "aligned_images = []\n",
    "#for r,d,f in os.walk(\"./dataset/img_align_celeba/\"):\n",
    "for r,d,f in os.walk(\"./resized_dataset/\"):\n",
    "    aligned_images = f\n",
    "    \n",
    "print(len(aligned_images))\n",
    "assert aligned_images\n",
    "\n",
    "\n",
    "\n",
    "#################\n",
    "\n",
    "# set cudnn benchmark=True ,for the best of the backend to find best hardware algorithms\n",
    "cudnn.benchmark = True\n",
    "\n",
    "generator = Generator(z_dim, num_gf, num_channels, use_cuda, num_gpu)\n",
    "discriminator = Discriminator(num_channels, num_df, use_cuda, num_gpu)\n",
    "\n",
    "# move generator and disciminator to cuda device if cuda is activated\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "# init weights of  g and d\n",
    "\n",
    "generator.apply(init_weights)\n",
    "discriminator.apply(init_weights)\n",
    "\n",
    "# load net state if exists\n",
    "print(\"load trained state dict from local files...\")\n",
    "generator.load_state_dict(torch.load(netG_path))\n",
    "discriminator.load_state_dict(torch.load(netD_path))\n",
    "print(\"generator and discriminator state dict loaded, done.\")\n",
    "\n",
    "print(\"Generator Info:\")\n",
    "print(generator)\n",
    "print(\"Discriminator Info:\")\n",
    "print(discriminator)\n",
    "\n",
    "image_shape = [num_channels, image_size, image_size]\n",
    "\n",
    "criteria = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_images = aligned_images[0:1000]\n",
    "for i in range(len(aligned_images)):\n",
    "    #aligned_images[i] = \"./dataset/img_align_celeba/\" + aligned_images[i]\n",
    "    \n",
    "    aligned_images[i] = \"./resized_dataset/resized_dataset/\" + aligned_images[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.resize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_img = imread(\"./qd_imd/train/14508_train.png\")\n",
    "mask_img = mask_img[:, :, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_img=np.swapaxes(mask_img,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 512, 512)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=np.vstack([mask_img]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.reshape(10,3,512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3, 512, 512)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=torch.from_numpy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.DoubleTensor'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.double().type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('test',mask_img)\n",
    "cv2.waitKey(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-7f5660ea3e52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmask\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'mask' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impainting(mask_tag=True):\n",
    "    # load mask\n",
    "    mask_img = imread(\"./qd_imd/train/29177_train.png\")\n",
    "    \n",
    "    #mask_img = mask_img[:, :, ::-1]\n",
    "    \n",
    "    mask_img_ds = resize(mask_img, (64,64))\n",
    "    mask_img_ds=mask_img_ds*1.0/255.0\n",
    "    # 创建输出文件夹\n",
    "    output_dirs = output_dir\n",
    "    source_imagedir = os.path.join(output_dirs, \"source_images\")\n",
    "    masked_imagedir = os.path.join(output_dirs, \"masked_images\")\n",
    "    impainted_imagedir = os.path.join(output_dirs, \"impainted_images\")\n",
    "    os.makedirs(source_imagedir, exist_ok=True)\n",
    "    os.makedirs(masked_imagedir,exist_ok=True)\n",
    "    os.makedirs(impainted_imagedir,exist_ok=True)\n",
    "\n",
    "    # 总共需要修复多少图片\n",
    "    num_images = len(aligned_images)\n",
    "    # 总共可以分为多少的batch来进行处理\n",
    "    num_batches = int(np.ceil(num_images / batch_size))\n",
    "\n",
    "    for idx in range(num_batches):\n",
    "        # 对于每一个batch的图片进行如下处理\n",
    "        lidx = idx * batch_size\n",
    "        hidx = min(num_images, (idx + 1) * batch_size)\n",
    "        realBatchSize = hidx - lidx\n",
    "\n",
    "        batch_images = [get_tensor_image(imgpath) for imgpath in aligned_images[lidx:hidx]]\n",
    "        batch_images = torch.stack(batch_images).to(device)\n",
    "        # if realBatchSize < args.batch_size:\n",
    "        #     print(\"number of batch images : \", realBatchSize)\n",
    "        #     # 如果需要修补的图片没有一个batch那么多，用0来填充\n",
    "        #     batch_images = np.pad(batch_images, [(0, args.batch_size - realBatchSize), (0, 0), (0, 0), (0, 0)], \"constant\")\n",
    "        #     batch_images = batch_images.astype(np.float32)\n",
    "        \n",
    "        # 输入的原始图片已经准备好，开始准备mask\n",
    "        # 暂时只提供中心mask\n",
    "        if not mask_tag:\n",
    "            mask = torch.ones(size=image_shape).to(device)\n",
    "            imageCenterScale = 0.3\n",
    "            lm = int(image_size * imageCenterScale)\n",
    "            hm = int(image_size * (1 - imageCenterScale))\n",
    "            # 将图像中心mask为0\n",
    "            mask[:,lm:hm, lm:hm] = 0.0\n",
    "        else:\n",
    "            mask_img_ds=np.swapaxes(mask_img_ds,0,2)\n",
    "            mask_img_ds=np.vstack([mask_img_ds]*batch_images.shape[0])\n",
    "            mask_img_ds=mask_img_ds.reshape(batch_images.shape[0],3,64,64)\n",
    "            mask_img_ds=torch.from_numpy(mask_img_ds).type(torch.FloatTensor)\n",
    "            #print(mask_img_ds)\n",
    "            mask=mask_img_ds.to(device)\n",
    "            \n",
    "        #print(batch_images.shape)\n",
    "        #print(mask.shape)\n",
    "        #return batch_images,mask\n",
    "        print(batch_images.shape)\n",
    "        print(mask.shape)\n",
    "        print(mask.type())\n",
    "        #print()\n",
    "        masked_batch_images = torch.mul(batch_images, mask).to(device)\n",
    "        #batch_images[:,:,32:96,32:96]=mask\n",
    "        #masked_batch_images=batch_images.to(device)\n",
    "\n",
    "        # 先保存一下原始图片和masked图片\n",
    "        save_tensor_images(batch_images.detach(),\n",
    "                   os.path.join(source_imagedir,\"source_image_batch_{}.png\".format(idx)))\n",
    "    \n",
    "        save_tensor_images(masked_batch_images.detach(), os.path.join(masked_imagedir, \"masked_image_batch_{}.png\".format(idx)))\n",
    "\n",
    "       \n",
    "        z_hat = torch.rand(size=[realBatchSize,z_dim,1,1],dtype=torch.float32,requires_grad=True,device=device)\n",
    "        z_hat.data.mul_(2.0).sub_(1.0)\n",
    "        opt = optim.Adam([z_hat],lr=0.0001)       \n",
    "        print(\"start impainting iteration for batch : {}\".format(idx))\n",
    "        v=torch.tensor(0,dtype=torch.float32,device=device)\n",
    "        m=torch.tensor(0,dtype=torch.float32,device=device)\n",
    "        \n",
    "        for iteration in range(num_iters):\n",
    "            # 对每一个batch的图像分别迭代impainting\n",
    "            if z_hat.grad is not None:\n",
    "                z_hat.grad.data.zero_()\n",
    "            generator.zero_grad()\n",
    "            discriminator.zero_grad()\n",
    "            batch_images_g = generator(z_hat)\n",
    "            batch_images_g_masked = torch.mul(batch_images_g,mask) \n",
    "            impainting_images = torch.mul(batch_images_g,(1-mask))+masked_batch_images\n",
    "            if iteration % 100==0:\n",
    "                # 保存impainting 图片结果\n",
    "                print(\"\\nsaving impainted images for batch: {} , iteration:{}\".format(idx,iteration))\n",
    "                save_tensor_images(impainting_images.detach(), os.path.join(impainted_imagedir,\"impainted_image_batch_{}_iteration_{}.png\".format(idx,iteration)))\n",
    "\n",
    "            loss_context = torch.norm(\n",
    "                (masked_batch_images-batch_images_g_masked),p=1)\n",
    "            dis_output = discriminator(impainting_images)\n",
    "#             print(dis_output)\n",
    "            batch_labels = torch.full((realBatchSize,), 1, device=device)\n",
    "            loss_perceptual = criteria(dis_output,batch_labels)\n",
    "            \n",
    "            total_loss = loss_context + lamd*loss_perceptual\n",
    "            print(\"\\r batch {} : iteration : {:4} , context_loss:{:.4f},percptual_loss:{:4f}\".format(idx,iteration,loss_context,loss_perceptual),end=\"\")\n",
    "            total_loss.backward()\n",
    "            opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 64, 64])\n",
      "torch.Size([64, 3, 64, 64])\n",
      "torch.cuda.FloatTensor\n",
      "start impainting iteration for batch : 0\n",
      "\n",
      "saving impainted images for batch: 0 , iteration:0\n",
      " batch 0 : iteration :   99 , context_loss:176201.0312,percptual_loss:1.677258\n",
      "saving impainted images for batch: 0 , iteration:100\n",
      " batch 0 : iteration :  199 , context_loss:171184.6094,percptual_loss:1.708680\n",
      "saving impainted images for batch: 0 , iteration:200\n",
      " batch 0 : iteration :  299 , context_loss:166381.4062,percptual_loss:1.708668\n",
      "saving impainted images for batch: 0 , iteration:300\n",
      " batch 0 : iteration :  399 , context_loss:162005.0938,percptual_loss:1.720596\n",
      "saving impainted images for batch: 0 , iteration:400\n",
      " batch 0 : iteration :  499 , context_loss:157782.2031,percptual_loss:1.739728\n",
      "saving impainted images for batch: 0 , iteration:500\n",
      " batch 0 : iteration :  599 , context_loss:153794.2344,percptual_loss:1.747076\n",
      "saving impainted images for batch: 0 , iteration:600\n",
      " batch 0 : iteration :  699 , context_loss:150052.3125,percptual_loss:1.769132\n",
      "saving impainted images for batch: 0 , iteration:700\n",
      " batch 0 : iteration :  799 , context_loss:146638.4844,percptual_loss:1.836550\n",
      "saving impainted images for batch: 0 , iteration:800\n",
      " batch 0 : iteration :  899 , context_loss:143368.5625,percptual_loss:1.886828\n",
      "saving impainted images for batch: 0 , iteration:900\n",
      " batch 0 : iteration :  999 , context_loss:140157.2812,percptual_loss:1.955426"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 50331648 into shape (64,3,64,64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-73075458021f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimpainting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-92-4993f230f101>\u001b[0m in \u001b[0;36mimpainting\u001b[1;34m(mask_tag)\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mmask_img_ds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask_img_ds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mmask_img_ds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask_img_ds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mmask_img_ds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask_img_ds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m             \u001b[0mmask_img_ds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask_img_ds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;31m#print(mask_img_ds)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 50331648 into shape (64,3,64,64)"
     ]
    }
   ],
   "source": [
    "impainting()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122880"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*64*3*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[:,:,32:96,32:96]=mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start impainting iteration for batch : 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (178) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-73075458021f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimpainting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-62218d6ddf6b>\u001b[0m in \u001b[0;36mimpainting\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mbatch_images_g\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mbatch_images_g_masked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_images_g\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0mimpainting_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_images_g\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmasked_batch_images\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[1;31m# 保存impainting 图片结果\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (178) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "impainting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=torch.from_numpy(np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[2]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1], dtype=torch.int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "s_img = cv2.imread(\"smaller_image.png\")\n",
    "l_img = cv2.imread(\"larger_image.jpg\")\n",
    "x_offset=y_offset=50\n",
    "l_img[y_offset:y_offset+s_img.shape[0], x_offset:x_offset+s_img.shape[1]] = s_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
